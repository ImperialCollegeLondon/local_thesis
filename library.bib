@inproceedings{bengioCurriculumLearning2009,
  title = {Curriculum Learning},
  booktitle = {Proceedings of the 26th {{Annual International Conference}} on {{Machine Learning}} - {{ICML}} '09},
  author = {Bengio, Yoshua and Louradour, J{\'e}r{\^o}me and Collobert, Ronan and Weston, Jason},
  year = {2009},
  pages = {1--8},
  publisher = {{ACM Press}},
  address = {{Montreal, Quebec, Canada}},
  doi = {10.1145/1553374.1553380},
  url = {http://portal.acm.org/citation.cfm?doid=1553374.1553380},
  urldate = {2021-03-26},
  abstract = {Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them ``curriculum learning''. In the context of recent research studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neural networks), we explore curriculum learning in various set-ups. The experiments show that significant improvements in generalization can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).},
  isbn = {978-1-60558-516-1},
  langid = {english}
}
